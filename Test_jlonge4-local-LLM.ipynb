{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c501fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import streamlit as st\n",
    "from langchain.llms.base import LLM\n",
    "from llama_index import LLMPredictor, LangchainEmbedding, ServiceContext, PromptHelper\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1381cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "MODEL_PATH = \"/home/yumk/Desktop/trained_qlora/llama2hf\"\n",
    "# Number of threads to use\n",
    "NUM_THREADS = 8\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 2048\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum chunk overlap\n",
    "chunk_overlap_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac63633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8866cb6298824659abbe5dc96c8f3136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2208b9de0c3a4aa4b2535ea726ddc2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e46ca29f96467cb107e17ba0dd6ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09194cc4a4d45638898c15c663ca3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a8730e0f164995ba8eca656495b497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df11b8d268964992b9eb89a34c3d55d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac58113a7eb443f8e12e2812c0683ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195a140e74d1439b882e62eb5d3d2521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed922d782a8e44b9bcf971d408275d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efa8a2bb8524f9491c3e2786163c4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ee29644b0443b297a5b59f09452d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d43ab49079148dba869702465ecffc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9d820e3ee4402897b9108991984a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3695b97bd7744ed8a8c36f5d366a1ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The prompt helper is needed to keep query and context inside of the models context window by reducing original text\n",
    "# to a ratio of its original size. (chunk_overlap_ratio). The model may process each of these chunks, and then reprocess\n",
    "# the final answer by combining each chunks response together. A larger chunk size is chosen initially to save\n",
    "# compute power, however if this exceeds the context window, the prompt will be retried with a smaller chunk ratio.\n",
    "\n",
    "try:\n",
    "    prompt_helper = PromptHelper(max_input_size, num_output, chunk_overlap_ratio)\n",
    "except Exception as e:\n",
    "    chunk_overlap_ratio = 0.2  # Set a different max_chunk_overlap value for the next attempt\n",
    "    prompt_helper = PromptHelper(max_input_size, num_output, chunk_overlap_ratio)\n",
    "    \n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
    "\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    model_name = MODEL_NAME\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        p = f\"Human: {prompt} Assistant: \"\n",
    "        prompt_length = len(p)\n",
    "        llm = Llama(model_path=MODEL_PATH, n_threads=NUM_THREADS)\n",
    "        output = llm(p, max_tokens=512, stop=[\"Human:\"], echo=True)['choices'][0]['text']\n",
    "        # only return newly generated tokens by slicing list to include words after the original prompt\n",
    "        response = output[prompt_length:]\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402efd97",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'streamlit' has no attribute 'init_session_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m         st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     st\u001b[38;5;241m.\u001b[39minit_session_state()\n\u001b[1;32m     19\u001b[0m     init()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;129m@st\u001b[39m\u001b[38;5;241m.\u001b[39mcache_resource\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_llm\u001b[39m():\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'streamlit' has no attribute 'init_session_state'"
     ]
    }
   ],
   "source": [
    "# define our LLM\n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)\n",
    "\n",
    "\n",
    "def clear_convo():\n",
    "    st.session_state['messages'] = []\n",
    "\n",
    "\n",
    "def init():\n",
    "    st.set_page_config(page_title='Local LLama', page_icon=':robot_face: ')\n",
    "    st.sidebar.title('Local LLama')\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state['messages'] = []\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()\n",
    "\n",
    "\n",
    "    @st.cache_resource\n",
    "    def get_llm():\n",
    "        llm = CustomLLM()\n",
    "        return llm\n",
    "\n",
    "    clear_button = st.sidebar.button(\"Clear Conversation\", key=\"clear\")\n",
    "    if clear_button:\n",
    "        clear_convo()\n",
    "\n",
    "    user_input = st.chat_input(\"Say something\")\n",
    "\n",
    "    if user_input:\n",
    "        llm = get_llm()\n",
    "        llm._call(prompt=user_input)\n",
    "\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94726723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
